<!--

author: Volker G. G√∂hler

email:  volker.goehler@informatik.tu-freiberg.de

version: 0.0.1

language: de

narrator: Deutsch Male

edit: true
date: 2026
icon: img/TUBAF_Logo_EN_blau.png

logo: 
attribute: 

comment: Distributed Software

import: https://raw.githubusercontent.com/liaScript/mermaid_template/master/README.md

link: ./styles.css

title: N8N Workflows REST 06

tags: Lehre, TUBAF

-->
[![LiaScript](https://raw.githubusercontent.com/LiaScript/LiaScript/master/badges/course.svg)](https://liascript.github.io/course/?https://raw.githubusercontent.com/vgoehler/introduction-to-n8n/refs/heads/main/lesson_06.md)

# n8n Workflows: Blockaufgabe -- Von REST APIs zu MCP und LLM Integration

Volker G√∂hler, TU Bergakademie Freiberg

------------------------------

![Welcome](https://n8n.io/brandguidelines/logo-dark.svg "n8n Logo [n8n.io](https://n8n.io/)")<!-- style="width:500px;" -->

> "Code" auf https://github.com/vgoehler/introduction-to-n8n als Open Educational Ressource.

----------------------------------------

## R√ºckblick

**Nachtrag vom letzten mal**<!-- class="head" -->

Was haben wir letztes Mal gemacht?

    {{1}}
- MCP Grundlagen
- n8n Workflows als MCP Tools
- Rick&Morty REST API und MCP kombinieren

## Blockaufgabe

<section class="flex-container">
<div class="colorbox colorbox--hints flex-child">
<div class="colorbox__title">
Schritte
</div>
- REST API aus √úbung 8
- an n8n anschlie√üen
- MCP daf√ºr nutzen
- LLM Zugang f√ºr n8n
</div>
<div class="colorbox colorbox--but flex-child">
<div class="colorbox__title">
Flowchart
</div>
```mermaid @mermaid
flowchart TB
    %% External systems
    LLM[LLM]

    subgraph ExREST[REST Interface]
        direction TB
        REST[External REST API]
        DATA[(External Datasource)]
    end

    %% n8n subflow
    subgraph N8N[n8n server]
        direction TB
        REST_IF[n8n REST Interface]
        MCP[n8n MCP Server]
        LLM_EXEC[n8n LLM Executor]

        MCP --> REST_IF 
        MCP --> LLM_EXEC
    end

    %% Main flow
    REST_IF <--> REST
    LLM <--> LLM_EXEC 

    %% External datasource interaction (bidirectional + dashed)
    DATA <--> REST

    %% Styles
    classDef external fill:#E0F2FE,stroke:#0284C7,stroke-width:1.5px,color:#0F172A;
    classDef datasource fill:#ECFEFF,stroke:#0891B2,stroke-width:1.5px,color:#0F172A;
    classDef n8nNode fill:#DBEAFE,stroke:#2563EB,stroke-width:1.5px,color:#0F172A;
    classDef llm fill:#EEF2FF,stroke:#4F46E5,stroke-width:1.5px,color:#0F172A;

    class REST external;
    class DATA datasource;
    class REST_IF,MCP,LLM_EXEC n8nNode;
    class LLM llm;

    style N8N fill:#EFF6FF,stroke:#2563EB,stroke-width:2px;
    style ExREST fill:#EFF6FF,stroke:#2563EB,stroke-width:2px;

```
</div>
</section>

### 1. Pal√§o REST API

In √úbung 8 haben Sie eine REST API geschrieben.

<div class="colorbox colorbox--hints" style="width:100%;">
<div class="colorbox__title">
Arbeitsanweisung
</div>
- holen Sie sich den Code von √úbung 8
- starten Sie diese lokal (nutzen Sie z.B. `uvicorn`)
- testen Sie die API mit einem REST Client (z.B. Postman, Insomnia, n8n HTTP Request Node, curl, ...)
</div>

<!-- class="lia-callout--note" -->
> L√∂sungscode von Herrn Vater: [√úbung 8 REST API](https://ificloud.xsitepool.tu-freiberg.de/index.php/s/ZtZo32PsDLdGioR)

```python main.py
#!/usr/bin/env python3

import uvicorn

def main():
    uvicorn.run(
            "fosil_rest_e08.server.8-1-Server:app",
            host="0.0.0.0",
            port=8008,
            reload=True,
            )
# Name ihrer App muss ggf. angepasst werden

if __name__ == "__main__":
    main()
```

### 2. Die REST API mit Daten f√ºttern

<section class="flex-container">

<div class="colorbox colorbox--but flex-child">
<div class="colorbox__title">
üîå Aufgabe: REST API im n8n-Workflow nutzen
</div>

- **Rufen Sie Daten** aus der digitalen pal√§ontologischen Sammlung der TUBAF ab  
- **Verarbeiten und speichern Sie** die Ergebnisse in Ihrem n8n-Workflow  
- üåê **Datenquelle:**  
  [Digitale Pal√§ontologische Sammlung der TUBAF](https://ifiweb.informatik.tu-freiberg.de/public/paleoweb/index.html)

- **API-Endpoint:**  
  `https://ifiweb.informatik.tu-freiberg.de/public/lehre/vs/api.php?action=fetch-model&model_id=<ID_NR>`

-  **Wichtig:**  
  `ID_NR` ist eine **positive Ganzzahl**, die **nicht kontinuierlich** vergeben ist
</div>
<div class="flex-child" style="width:100%;text-align:center;">
![](img/fosil.png "Harpes sp. ‚Äì Fossilmodell (TUBAF)")<!-- style="margin-top: 6rem;" -->
</div>
</section>

<div class="colorbox colorbox--steps" style="width:100%;margin-top:1em;">
<div class="colorbox__title">
üí° Hinweise & Leitfragen
</div>

- ‚ùì **Fehlerbehandlung:**  
  Wie reagiert der Endpoint bei ung√ºltigen oder nicht existierenden IDs?

- üíæ **Persistierung:** Was passiert wenn der REST-Server neu gestartet wird? Ans√§tze:

    {{1}}
   - **In n8n:**  
    Data Tables, Dateien oder Workflow-Zustand
   - **Serverseitig (Datenbank):**  
    Erweiterung des API-Servers um eine Datenbank  
    (z. B. SQLite oder eine verteilte Datenbank)
   - **Serverseitig (Serialisierung):**  
    Persistieren der In-Memory-Daten (z. B. als JSON)  
    und erneutes Laden beim Start des Servers

- üîç **Zustands√ºberwachung:**  
  Wie erkennen Sie bereits gelesene Fossilien?  
  Gibt es Strategien f√ºr Updates oder erneute Abfragen?
</div>

### 3.1. LLM Integration in n8n

<div class="colorbox colorbox--batch" style="width:100%;margin-top:1em;">
<div class="colorbox__title">
Aufgabe: LLMs in n8n integrieren
</div>

- üéØ **Ziel:** Binden Sie ein Large Language Model (LLM) in einen n8n-Workflow ein.
- **Integration in n8n:**  

  - native LLM-Nodes (z. B. OpenAI, Ollama)  
  - alternativ √ºber den **HTTP Request Node** via REST API

- **Zwei grundlegende Betriebsmodelle:**

  - üåê **Cloud-basiert**
  - üè† **Lokal (Self-Hosting)**

</div>

#### üåê Cloud LLM mit API Key

Georg-August-Universit√§t G√∂ttingen: ([Chat-AI](https://www.uni-goettingen.de/de/686446.html))

<div class="colorbox colorbox--hints" style="width:100%;">
<div class="colorbox__title">
Zugriff auf Chat-AI der Universit√§t G√∂ttingen
</div>
- Ein **API-Key** wird bereitgestellt.
- Integration in n8n √ºber den **OpenAI Node**:

  - Endpoint setzen auf `https://chat-ai.academiccloud.de/v1`
  - Modellnamen aus der [Chat-AI Modell√ºbersicht](https://docs.hpc.gwdg.de/services/chat-ai/models/index.html)
  - **Wichtig:** Im Node das Dropdown auf *ID* stellen und Modell-ID manuell eintragen

- ‚ö†Ô∏è **Hinweis:**  
  Der API-Key ist **kontingentiert** (Anfragen pro Monat).  
  ‚Üí Bitte bewusst und sparsam einsetzen.
</div>

<!-- class="subhead" -->
Beispielcode f√ºr die Nutzung der Chat-AI API mit Python:

```python connect_to_ChatAI.py
import os
from openai import OpenAI

API_KEY = os.environ.get("CHAT_AI_KEY")
API_URL = "https://chat-ai.academiccloud.de/v1"


client = OpenAI(
        api_key=API_KEY,
        base_url=API_URL
        )

response = client.responses.create(
        model="mistral-large-instruct",
        instructions="You are a professional teacher. Your answers should be didactic and detailed.",
        input="What is the capital of France?",
        )

print(response.output_text)
```

#### üè† Lokales Self-Hosting von LLMs

- Wir nutzen **Ollama** [https://ollama.com/](https://ollama.com/)
- Eine Open-Source Plattform zum **lokalen Verwalten und Ausf√ºhren** von LLMs
- Mittels eines CLI Tools werden Modelle lokal geladen und √ºber eine REST-Schnittstelle bereitgestellt

<div class="colorbox colorbox--hints" style="width:100%">
<div class="colorbox__title">
üîß **Grundlegende Schritte:**
</div>
- Installation von Ollama
- Modell laden oder starten

   - `ollama run mistral`  
   - `ollama pull llama3.3`

- LLM erreichbar unter: `http://localhost:11434`
- Integration in n8n √ºber den **Ollama Node**
</div>

<!-- class="lia-callout--note" -->
> ‚ö†Ô∏è **Docker-Hinweis:**  \
Bei n8n im Docker-Container muss die **IP des Host-Rechners** verwendet werden.

<div class="colorbox colorbox--steps" style="width:100%;margin-top:1em;">
<div class="colorbox__title">
üîç Hinweise & Abw√§gungen
</div>

- **Hardware-Anforderungen:**  
  GPU (VRAM), RAM und CPU pr√ºfen
- **Empfehlung:**  
  Bei begrenzten Ressourcen kleinere, effiziente Modelle nutzen  
  (z. B. `Mistral-7B-v0.3`, `Llama3.2-3B`)
- **Abw√§gung:**  
  Cloud ‚Üí einfach, leistungsf√§hig, eventuell teuer
  Lokal ‚Üí kontrolliert, datenschutzfreundlich
</div>

Das lokal gehostete LLM kann direkt √ºber die Ollama-REST-API angesprochen werden.

```python ollama_request.py
import requests
import json

OLLAMA_URL = "http://localhost:11434/api/generate"

payload = {
    "model": "mistral",
    "prompt": "Explain the concept of REST APIs in simple terms.",
    "stream": False
}

response = requests.post(
    OLLAMA_URL,
    headers={"Content-Type": "application/json"},
    data=json.dumps(payload)
)

response.raise_for_status()

result = response.json()
print(result["response"])
```

### 3.2. MCP Integration

<section class="flex-container">

<div class="colorbox colorbox--but flex-child" style="width:100%;">
<div class="colorbox__title">
üß© Aufgabe: REST API als MCP-Tools in n8n integrieren
</div>

- üéØ **Ziel:** Stellen Sie die Funktionen der REST API als **MCP-Tools** bereit, sodass ein LLM sie in n8n kontrolliert nutzen kann.
- **Denken Sie in Tools, nicht in Endpoints:** Ein Tool entspricht einer klaren Aktion (z. B. *"Fossilmodell abrufen"*), nicht einer generischen HTTP-Anfrage.
- Was ben√∂tigt das LLM an Informationen? Welche Tools sind sinnvoll?
- **Mindestanforderung:** Implementieren Sie mindestens **ein Tool** (lesend), das direkt auf den REST-Endpoint zugreift.
</div>

<div class="colorbox colorbox--errorwf flex-child" style="width:100%;">
<div class="colorbox__title">
üß≠ Hinweise & Leitfragen (f√ºr Tool-Design)
</div>

- **Parameter-Design:**  
  Welche Parameter braucht das Tool? Datentypen? Validierungen?

- **R√ºckgabeformat:**  
  Was soll das Tool zur√ºckgeben, damit das LLM sinnvoll weiterarbeiten kann? strukturierte JSON-Daten, Statusfelder, Fehlertypen

- **Fehlerbehandlung:**  
  Wie unterscheiden Sie **"nicht gefunden"** (404), **"ung√ºltige Eingabe"** (400) und **"Serverfehler"** (5xx)? Welche Fehlermeldung ist f√ºr das LLM am hilfreichsten?

- ‚úÖ **Testf√§lle (Kurzcheck):**

   - g√ºltige ID **‚Üí** Erfolg
   - nicht existierende ID **‚Üí** definierter Fehler / leere Antwort
   - ung√ºltige ID (z. B. Text) **‚Üí** validierter Parameterfehler
</div>

</section>

MCP nutzt das JSON-RPC 2.0 Format f√ºr die Kommunikation. Fehler sind dabei wie folgt zu melden:

```json
{
  "jsonrpc": "2.0",
  "id": "request-123",
  "error": {
    "code": -32601,
    "message": "Method not found",
    "data": "The method 'unknown_tool' does not exist"
  }
}
```
[Source](https://mcpcat.io/guides/error-handling-custom-mcp-servers/) \
Das Format nutzt eigene Fehlercodes, Sie k√∂nnen aber auf HTTP Fehlercodes aufbauen.



## Reflexion & Fazit

<section class="flex-container">

<div class="flex-child">

<div class="colorbox colorbox--hints" style="width:100%;">
<div class="colorbox__title">
üß≠ Reflexion
</div>

- Wie ver√§ndert sich die Rolle von **Workflows**, wenn LLMs Entscheidungen treffen?
- Welche Verantwortung liegt beim **Systemdesign** ‚Äì nicht beim Modell?
- Wo sind klare **Grenzen** zwischen Logik, Daten und KI sinnvoll?
</div>

<div class="colorbox colorbox--but" style="width:100%;margin-top:1em;">
<div class="colorbox__title">
‚úÖ Fazit
</div>

- n8n erm√∂glicht die **strukturierte Orchestrierung** von APIs, Daten und LLMs
- es erm√∂glicht einfach automatisierte Workflows mit **geringerem Code-Aufwand**
- MCP schafft eine **klare Schnittstelle** zwischen LLMs und Funktionalit√§t
- die Kombination er√∂ffnet **neue M√∂glichkeiten** f√ºr intelligente Automatisierung

</div>

</div>

<div class="flex-child" style="width:100%;text-align:center;">
![](img/network.jpg "Network, Image bei OpenAI")
</div>

</section>
